# æµå¼è¾“å‡ºæ¶æ„æ›´æ–° - StreamChunk å®ç°

## ğŸ¯ æ¶æ„æ”¹è¿›æ¦‚è¿°

æ ¹æ®ä½ çš„å»ºè®®ï¼Œæˆ‘ä»¬å°†æµå¼è¾“å‡ºæ¶æ„ä»ç›´æ¥ yield å­—ç¬¦ä¸²å†…å®¹å‡çº§ä¸ºä½¿ç”¨ç»“æ„åŒ–çš„ `StreamChunk` å¯¹è±¡ï¼Œè¿™æä¾›äº†æ›´å¥½çš„å¯æ‰©å±•æ€§å’Œå…ƒæ•°æ®æ”¯æŒã€‚

## ğŸ—ï¸ æ–°æ¶æ„è®¾è®¡

### 1. StreamChunk å¯¹è±¡

```python
@dataclass
class StreamChunk:
    """å•ä¸ªæµå¼è¾“å‡ºå—"""
    content: str
    chunk_type: str = "text"  # text, tool_call, observation, error
    metadata: Dict[str, Any] = field(default_factory=dict)
    timestamp: float = field(default_factory=time.time)
```

### 2. åˆ†å±‚æ¶æ„

```
ç”¨æˆ·åº”ç”¨å±‚
    â†“ (å¤„ç† StreamChunk å¯¹è±¡)
Minion å±‚ (WorkerMinion._process_stream_generator)
    â†“ (åˆ›å»º StreamChunk å¯¹è±¡)
ActionNode å±‚ (LmpActionNode._execute_stream_generator)
    â†“ (yield å­—ç¬¦ä¸²å†…å®¹)
LLM Provider å±‚ (OpenAIProvider.generate_stream)
    â†“ (yield å­—ç¬¦ä¸²å†…å®¹)
OpenAI API
```

## ğŸ”§ å®ç°ç»†èŠ‚

### 1. LLM Provider å±‚
- **ä¿æŒä¸å˜**: ç»§ç»­ yield å­—ç¬¦ä¸²å†…å®¹
- **åŸå› **: ä¿æŒåº•å±‚ç®€å•æ€§ï¼Œä¸“æ³¨äº LLM äº¤äº’

```python
# OpenAI Provider
async def generate_stream(self, messages, **kwargs) -> AsyncIterator[str]:
    async for chunk in self.generate_stream_chunk(messages, **kwargs):
        if chunk.choices[0].delta.content:
            content = chunk.choices[0].delta.content
            yield content  # ç›´æ¥ yield å­—ç¬¦ä¸²
```

### 2. ActionNode å±‚
- **ä¿æŒä¸å˜**: ç»§ç»­ yield å­—ç¬¦ä¸²å†…å®¹
- **åŸå› **: ä¿æŒ ActionNode çš„é€šç”¨æ€§

```python
# LmpActionNode
async def _execute_stream_generator(self, messages, **api_params):
    async for chunk in self.llm.generate_stream(messages, **api_params):
        yield chunk  # ç›´æ¥ä¼ é€’å­—ç¬¦ä¸²
```

### 3. Minion å±‚ (å…³é”®æ”¹è¿›)
- **æ–°å¢**: åœ¨ `_process_stream_generator` ä¸­åˆ›å»º `StreamChunk` å¯¹è±¡
- **ä¼˜åŠ¿**: æ·»åŠ å…ƒæ•°æ®å’Œç»“æ„åŒ–ä¿¡æ¯

```python
# WorkerMinion
async def _process_stream_generator(self, stream_generator):
    from minion.main.action_step import StreamChunk
    
    full_response = ""
    chunk_counter = 0
    
    async for chunk in stream_generator:
        content = str(chunk)
        full_response += content
        chunk_counter += 1
        
        # åˆ›å»º StreamChunk å¯¹è±¡
        stream_chunk = StreamChunk(
            content=content,
            chunk_type="text",
            metadata={
                "minion_type": self.__class__.__name__,
                "chunk_number": chunk_counter,
                "total_length": len(full_response)
            }
        )
        yield stream_chunk
```

### 4. ç”¨æˆ·åº”ç”¨å±‚
- **æ–°å¢**: å¤„ç† `StreamChunk` å¯¹è±¡
- **å‘åå…¼å®¹**: åŒæ—¶æ”¯æŒå­—ç¬¦ä¸²å’Œ `StreamChunk`

```python
# æ¼”ç¤ºä»£ç 
async for chunk in stream_generator:
    # å¤„ç† StreamChunk å¯¹è±¡æˆ–å­—ç¬¦ä¸²
    if hasattr(chunk, 'content'):
        content = chunk.content
        # å¯ä»¥è®¿é—®é¢å¤–çš„å…ƒæ•°æ®
        metadata = chunk.metadata
        chunk_type = chunk.chunk_type
    else:
        content = str(chunk)
    
    print(content, end='', flush=True)
```

## ğŸ“Š æ¶æ„ä¼˜åŠ¿

### 1. ç»“æ„åŒ–æ•°æ®
- **å…ƒæ•°æ®æ”¯æŒ**: æ¯ä¸ªå—åŒ…å«ä¸°å¯Œçš„å…ƒæ•°æ®ä¿¡æ¯
- **ç±»å‹æ ‡è¯†**: æ”¯æŒä¸åŒç±»å‹çš„å—ï¼ˆæ–‡æœ¬ã€å·¥å…·è°ƒç”¨ã€è§‚å¯Ÿç­‰ï¼‰
- **æ—¶é—´æˆ³**: è‡ªåŠ¨è®°å½•æ¯ä¸ªå—çš„ç”Ÿæˆæ—¶é—´

### 2. å¯æ‰©å±•æ€§
- **æœªæ¥æ‰©å±•**: å¯ä»¥è½»æ¾æ·»åŠ æ–°çš„å—ç±»å‹å’Œå…ƒæ•°æ®
- **è°ƒè¯•æ”¯æŒ**: å…ƒæ•°æ®æœ‰åŠ©äºè°ƒè¯•å’Œç›‘æ§
- **ç»Ÿè®¡åˆ†æ**: å¯ä»¥æ”¶é›†è¯¦ç»†çš„æµå¼è¾“å‡ºç»Ÿè®¡ä¿¡æ¯

### 3. å‘åå…¼å®¹
- **æ¸è¿›å‡çº§**: ç°æœ‰ä»£ç å¯ä»¥ç»§ç»­å·¥ä½œ
- **çµæ´»å¤„ç†**: åŒæ—¶æ”¯æŒå­—ç¬¦ä¸²å’Œ `StreamChunk` å¯¹è±¡

## ğŸ”„ è¿ç§»æŒ‡å—

### å¯¹äºç°æœ‰ä»£ç 
```python
# æ—§ä»£ç  (ä»ç„¶å·¥ä½œ)
async for chunk in stream_generator:
    print(chunk, end='', flush=True)

# æ–°ä»£ç  (æ¨è)
async for chunk in stream_generator:
    if hasattr(chunk, 'content'):
        content = chunk.content
    else:
        content = str(chunk)
    print(content, end='', flush=True)
```

### å¯¹äºæ–°åŠŸèƒ½
```python
async for chunk in stream_generator:
    if hasattr(chunk, 'content'):
        # è®¿é—®ç»“æ„åŒ–æ•°æ®
        print(f"[{chunk.chunk_type}] {chunk.content}", end='')
        
        # ä½¿ç”¨å…ƒæ•°æ®
        if chunk.metadata.get('chunk_number') == 1:
            print("\\n[é¦–ä¸ªå“åº”å—]", end='')
    else:
        print(str(chunk), end='')
```

## ğŸ§ª æµ‹è¯•éªŒè¯

æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡ï¼ŒåŒ…æ‹¬ï¼š
- âœ… å¯¼å…¥æµ‹è¯• - æ‰€æœ‰æ¼”ç¤ºæ–‡ä»¶æ­£å¸¸å¯¼å…¥
- âœ… åŸºæœ¬é€»è¾‘æµ‹è¯• - æµå¼è¾“å‡ºå’Œæ™®é€šè¾“å‡ºéƒ½æ­£å¸¸å·¥ä½œ
- âœ… Minion æµ‹è¯• - å„ç§ Minion çš„æµå¼è¾“å‡ºåŠŸèƒ½æ­£å¸¸
- âœ… æ¼”ç¤ºç±»æµ‹è¯• - æ¼”ç¤ºç±»åˆ›å»ºå’Œåˆå§‹åŒ–æ­£å¸¸

## ğŸš€ æœªæ¥æ‰©å±•æ–¹å‘

### 1. æ›´å¤šå—ç±»å‹
```python
# å·¥å…·è°ƒç”¨å—
StreamChunk(content="è°ƒç”¨å·¥å…·: search", chunk_type="tool_call")

# è§‚å¯Ÿç»“æœå—  
StreamChunk(content="æœç´¢ç»“æœ: ...", chunk_type="observation")

# é”™è¯¯å—
StreamChunk(content="é”™è¯¯ä¿¡æ¯", chunk_type="error")
```

### 2. æ›´ä¸°å¯Œçš„å…ƒæ•°æ®
```python
StreamChunk(
    content="...",
    metadata={
        "model": "gpt-4o",
        "temperature": 0.7,
        "token_count": 150,
        "confidence": 0.95,
        "processing_time": 0.1
    }
)
```

### 3. æµå¼æ§åˆ¶
```python
# æš‚åœ/æ¢å¤æµå¼è¾“å‡º
StreamChunk(content="", chunk_type="control", metadata={"action": "pause"})

# è¿›åº¦æŒ‡ç¤º
StreamChunk(content="", chunk_type="progress", metadata={"progress": 0.5})
```

## ğŸ“ æ€»ç»“

è¿™æ¬¡æ¶æ„æ›´æ–°æˆåŠŸåœ°å°†æµå¼è¾“å‡ºä»ç®€å•çš„å­—ç¬¦ä¸²æµå‡çº§ä¸ºç»“æ„åŒ–çš„ `StreamChunk` å¯¹è±¡æµï¼ŒåŒæ—¶ä¿æŒäº†ï¼š

1. **åº•å±‚ç®€å•æ€§** - LLM Provider å’Œ ActionNode å±‚ä¿æŒç®€å•
2. **å‘åå…¼å®¹æ€§** - ç°æœ‰ä»£ç æ— éœ€ä¿®æ”¹å³å¯å·¥ä½œ
3. **å¯æ‰©å±•æ€§** - ä¸ºæœªæ¥åŠŸèƒ½æ‰©å±•å¥ å®šäº†åŸºç¡€
4. **è°ƒè¯•å‹å¥½æ€§** - æä¾›äº†ä¸°å¯Œçš„å…ƒæ•°æ®æ”¯æŒ

è¿™ä¸ªè®¾è®¡éµå¾ªäº†"åœ¨æ­£ç¡®çš„å±‚æ¬¡åšæ­£ç¡®çš„äº‹"çš„åŸåˆ™ï¼Œåœ¨ Minion å±‚è¿›è¡Œæ•°æ®ç»“æ„åŒ–ï¼Œä¸ºä¸Šå±‚åº”ç”¨æä¾›æ›´å¥½çš„å¼€å‘ä½“éªŒã€‚