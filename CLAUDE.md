## ğŸ§  **è®°å¿†å­˜å‚¨**

### **ç³»ç»Ÿæ¶æ„è®°å¿†**
- never put test in the top level folder
- agent.run_async() è¿”å›çš„æ˜¯ä¸€ä¸ªasyncå‡½æ•°ï¼Œéœ€è¦å…ˆawaitæ‰èƒ½è·å¾—async generator
  - æ­£ç¡®ç”¨æ³•: `async for event in (await agent.run_async(input_obj, **kwargs)):`
  - é”™è¯¯ç”¨æ³•: `async for event in agent.run_async(input_obj, **kwargs):`
  - è¿™æ˜¯å› ä¸ºrun_asyncæ˜¯asyncå‡½æ•°ï¼Œå®ƒdelegateåˆ°å…¶ä»–å‡½æ•°ï¼Œæœ¬èº«éœ€è¦awaitæ‰èƒ½è¿”å›çœŸæ­£çš„async generator

- Agentæ„é€ å‡½æ•°è®¾è®¡æ¨¡å¼
  - æ‰€æœ‰Agentåº”ç»§æ‰¿BaseAgentå¹¶ä½¿ç”¨@dataclassè£…é¥°å™¨
  - æ„é€ å‡½æ•°å‚æ•°åº”ä¸BaseAgentå¯¹é½ï¼Œä½¿ç”¨dataclasså­—æ®µè€Œé__init__æ–¹æ³•
  - streamç›¸å…³å‚æ•°ä¸åº”åœ¨æ„é€ å‡½æ•°ä¸­ï¼Œè€Œæ˜¯é€šè¿‡run_async(stream=True/False)åŠ¨æ€æ§åˆ¶

- LLMæ„é€ å’Œè·å–æœ€ä½³å®è·µ
  - MinionToolCallingAgentæ„é€ æ—¶ä¼šè‡ªåŠ¨ä»modelé…ç½®åˆ›å»ºLLM
  - æ ‡å‡†LLMè·å–æ¨¡å¼ï¼ˆå‚è€ƒbrain.pyï¼‰ï¼š
    ```python
    # æ–¹å¼1ï¼šç›´æ¥æŒ‡å®šmodelåç§°ï¼Œä»config.modelsè·å–é…ç½®
    model = "gpt-4o"  # æˆ–å…¶ä»–æ¨¡å‹: "gemini-2.0-flash-exp", "deepseek-r1", "phi-4", "llama3.2"
    llm_config = config.models.get(model)
    llm = create_llm_provider(llm_config)
    
    # æ–¹å¼2ï¼šä½¿ç”¨é»˜è®¤æ¨¡å‹
    llm = create_llm_provider(config.models.get("default"))
    
    # æ–¹å¼3ï¼šåœ¨Agentæ„é€ æ—¶ä¼ å…¥modelåç§°ï¼Œè®©Agentè‡ªåŠ¨åˆ›å»º
    agent = MinionToolCallingAgent(model="gpt-4o")  # ä¼šè‡ªåŠ¨åˆ›å»ºLLM
    
    # ä½¿ç”¨dataclassé£æ ¼æ„é€ 
    agent = MinionToolCallingAgent(
        name="my_agent",
        tools=[tool1, tool2],
        model="gpt-4o",
        max_tool_threads=4
    )
    ```
  - Brainç±»LLMå¤„ç†é€»è¾‘ï¼šæ”¯æŒå­—ç¬¦ä¸²modelåç§°æˆ–ç›´æ¥ä¼ å…¥LLMå®ä¾‹
    - å¦‚æœllmå‚æ•°æ˜¯å­—ç¬¦ä¸²ï¼Œä¼šè°ƒç”¨`create_llm_provider(config.models.get(llm))`
    - å¦‚æœllmå‚æ•°æ˜¯LLMå®ä¾‹ï¼Œç›´æ¥ä½¿ç”¨
    - æ”¯æŒllmså­—å…¸æ‰¹é‡å¤„ç†å¤šä¸ªæ¨¡å‹é…ç½®

- Pythonæ‰§è¡Œç¯å¢ƒå’Œ<id>metadataå¤„ç†
  - åªæœ‰RpycPythonEnvéœ€è¦<id>metadataï¼Œå› ä¸ºå®ƒè¿æ¥docker/utils/python_server.py
  - LocalPythonEnv, LocalPythonExecutor, AsyncPythonExecutoréƒ½ä¸éœ€è¦<id>metadata
  - worker.pyå’Œbrain.pyä¸­ä¼šè‡ªåŠ¨æ£€æµ‹ç¯å¢ƒç±»å‹ï¼Œåªç»™RpycPythonEnvæ·»åŠ <id>ï¼š
    ```python
    # åªæœ‰RpycPythonEnvéœ€è¦<id>æ ‡ç­¾
    if self.python_env.__class__.__name__ == 'RpycPythonEnv':
        code_with_id = f"<id>{self.input.query_id}/{self.input.run_id}</id>{code}"
        result = self.python_env.step(code_with_id)
    else:
        # å…¶ä»–ç¯å¢ƒä¸éœ€è¦<id>æ ‡ç­¾
        result = self.python_env.step(code)
    ```

- functions.final_answerè°ƒç”¨ä¿®å¤
  - ä¿®å¤äº†`functions.final_answer()`è°ƒç”¨ä¸æŠ›å¼‚å¸¸çš„é—®é¢˜
  - é—®é¢˜åŸå› ï¼š`functions`å‘½åç©ºé—´ä¸­çš„`final_answer`æ˜¯åŸå§‹ç‰ˆæœ¬ï¼Œä¸ä¼šæŠ›å‡ºFinalAnswerException
  - è§£å†³æ–¹æ¡ˆï¼šåœ¨`evaluate_async_python_code`ä¸­åˆ›å»ºå¼‚å¸¸åŒ…è£…å™¨åï¼ŒåŒæ—¶æ›´æ–°`functions`å‘½åç©ºé—´
  - ç°åœ¨`functions.final_answer()`å’Œç›´æ¥è°ƒç”¨`final_answer()`éƒ½ä¼šæ­£ç¡®æŠ›å‡ºå¼‚å¸¸å¹¶è®¾ç½®`is_final_answer=True`

- worker.pyç»ˆæ­¢é€»è¾‘ä¿®å¤  
  - ä¿®å¤äº†Python executorè¿”å›`is_final_answer=True`ä½†ä»»åŠ¡ä¸ç»ˆæ­¢çš„é—®é¢˜
  - é—®é¢˜åŸå› ï¼šworker.pyè·å–äº†`is_final_answer`å€¼ä½†æ²¡æœ‰ä½¿ç”¨ï¼Œç¡¬ç¼–ç `terminated=False`
  - è§£å†³æ–¹æ¡ˆï¼šå½“`is_final_answer=True`æ—¶ç«‹å³è¿”å›`terminated=True`çš„AgentResponse
  - ç°åœ¨final_answerå·¥å…·è°ƒç”¨ä¼šæ­£ç¡®ç»ˆæ­¢ä»»åŠ¡æ‰§è¡Œ

- Minionæµå¼å¤„ç†é‡æ„
  - åœ¨åŸºç±»Minionä¸­æ·»åŠ äº†`stream_node_execution`é€šç”¨æ–¹æ³•
  - æ‰€æœ‰å­ç±»ç°åœ¨ä½¿ç”¨ç»Ÿä¸€çš„æµå¼å¤„ç†é€»è¾‘ï¼Œç›´æ¥yield StreamChunkå¯¹è±¡
  - ç§»é™¤äº†é”™è¯¯çš„final_answeræ£€æµ‹é€»è¾‘ï¼Œfinal_answerå¤„ç†ç”±LmpActionNodeè´Ÿè´£
  - ä¿æŒStreamChunkå¯¹è±¡çš„åŸå§‹ç»“æ„ï¼Œä¾¿äºä¸Šå±‚UIæ­£ç¡®å¤„ç†ä¸åŒç±»å‹çš„chunk

- tool_choiceå‚æ•°æ”¯æŒ
  - MinionToolCallingAgentå’ŒLmpActionNodeç°åœ¨éƒ½æ”¯æŒtool_choiceå‚æ•°
  - å¯é€‰å€¼ï¼š
    - "auto": è®©æ¨¡å‹è‡ªåŠ¨å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·ï¼ˆé»˜è®¤å€¼ï¼‰
    - "none": å¼ºåˆ¶æ¨¡å‹ä¸è°ƒç”¨ä»»ä½•å·¥å…·
    - {"type": "function", "function": {"name": "function_name"}}: å¼ºåˆ¶è°ƒç”¨ç‰¹å®šå·¥å…·
  - ä½¿ç”¨ç¤ºä¾‹ï¼š
    ```python
    # åœ¨Agentä¸­ä½¿ç”¨
    response = await agent.execute_step(state, tool_choice="none")
    
    # åœ¨LmpActionNodeä¸­ä½¿ç”¨
    response = await lmp_node.execute(messages, tools=tools, tool_choice="auto")
    
    # å¼ºåˆ¶è°ƒç”¨ç‰¹å®šå·¥å…·
    response = await lmp_node.execute(messages, tools=tools, 
                                    tool_choice={"type": "function", "function": {"name": "search"}})
    ```

- StreamChunkæµå¼å¤„ç†æœ€ä½³å®è·µ
  - StreamChunkæ˜¯agentæµå¼è¾“å‡ºçš„åŸºæœ¬å•å…ƒï¼Œæ¯ä¸ªchunkåŒ…å«ä¸€å°éƒ¨åˆ†å†…å®¹ï¼ˆå¦‚å•ä¸ªtokenï¼‰
  - UIå¤„ç†æ—¶åº”è¯¥ç´¯ç§¯StreamChunkå†…å®¹ï¼Œè€Œä¸æ˜¯ä¸ºæ¯ä¸ªchunkåˆ›å»ºå•ç‹¬çš„æ¶ˆæ¯è¡Œ
  - StreamChunkæœ‰ä¸åŒç±»å‹ï¼ˆchunk_typeï¼‰ï¼š
    - 'text': Providerå±‚è¾“å‡ºæ–‡æœ¬ï¼ˆopenai_providerä½¿ç”¨ï¼‰
    - 'tool_call': å·¥å…·è°ƒç”¨ä¿¡æ¯
    - 'tool_response': å·¥å…·å“åº”ç»“æœ
    - 'llm_output': Agentå±‚å¸¸è§„LLMè¾“å‡ºå†…å®¹
    - 'step_start': Agentæ­¥éª¤å¼€å§‹
    - 'step_end': Agentæ­¥éª¤ç»“æŸ
    - 'completion': ä»»åŠ¡å®Œæˆ
    - 'warning': è­¦å‘Šä¿¡æ¯
    - 'error': é”™è¯¯ä¿¡æ¯
    - 'final_answer': æœ€ç»ˆç­”æ¡ˆ
  - ç´¯ç§¯æ¨¡å¼å¤„ç†ï¼š
    ```python
    streaming_content = []
    if isinstance(event, StreamChunk):
        # å¯¹äºtextå’Œllm_outputç±»å‹çš„chunkéœ€è¦ç´¯ç§¯
        if event.chunk_type in ['text', 'llm_output']:
            streaming_content.append(event.content)
            display_content = "".join(streaming_content)
        else:
            # å…¶ä»–ç±»å‹å•ç‹¬å¤„ç†ï¼ˆå¦‚error, final_answerç­‰ï¼‰
            handle_special_chunk(event)
    ```

### **å¼€å‘æµç¨‹è®°å¿†**
- å¦‚æœæ˜¯ä¸€å®šåŠŸèƒ½çš„ä¿®æ”¹çš„è¯,å°½å¯èƒ½æ·»åŠ test,å…ˆè·‘é€štest
- å¦‚æœéå¸¸ç®€å•çš„ä¿®æ”¹å¯ä»¥ä¸ç”¨test
- å¦å¤–æ·»åŠ çš„æ–‡ä»¶è¯·git add åˆ°ç‰ˆæœ¬åº“é‡Œ
- å¦‚æœæ˜¯ä¿®æ”¹examples,ä¹Ÿè¯·æ”¹å®Œä¹‹åè¯•è·‘è¯¥example,ä¿®æ”¹å¿…è¦çš„bug,ç›´åˆ°è·‘é€šä¸ºæ­¢
