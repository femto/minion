## 🧠 **记忆存储**

### **系统架构记忆**
- never put test in the top level folder
- agent.run_async() 返回的是一个async函数，需要先await才能获得async generator
  - 正确用法: `async for event in (await agent.run_async(input_obj, **kwargs)):`
  - 错误用法: `async for event in agent.run_async(input_obj, **kwargs):`
  - 这是因为run_async是async函数，它delegate到其他函数，本身需要await才能返回真正的async generator

- Agent构造函数设计模式
  - 所有Agent应继承BaseAgent并使用@dataclass装饰器
  - 构造函数参数应与BaseAgent对齐，使用dataclass字段而非__init__方法
  - stream相关参数不应在构造函数中，而是通过run_async(stream=True/False)动态控制

- LLM构造和获取最佳实践
  - MinionToolCallingAgent构造时会自动从model配置创建LLM
  - 标准LLM获取模式（参考brain.py）：
    ```python
    # 方式1：直接指定model名称，从config.models获取配置
    model = "gpt-4o"  # 或其他模型: "gemini-2.0-flash-exp", "deepseek-r1", "phi-4", "llama3.2"
    llm_config = config.models.get(model)
    llm = create_llm_provider(llm_config)
    
    # 方式2：使用默认模型
    llm = create_llm_provider(config.models.get("default"))
    
    # 方式3：在Agent构造时传入model名称，让Agent自动创建
    agent = MinionToolCallingAgent(model="gpt-4o")  # 会自动创建LLM
    
    # 使用dataclass风格构造
    agent = MinionToolCallingAgent(
        name="my_agent",
        tools=[tool1, tool2],
        model="gpt-4o",
        max_tool_threads=4
    )
    ```
  - Brain类LLM处理逻辑：支持字符串model名称或直接传入LLM实例
    - 如果llm参数是字符串，会调用`create_llm_provider(config.models.get(llm))`
    - 如果llm参数是LLM实例，直接使用
    - 支持llms字典批量处理多个模型配置

- Python执行环境和<id>metadata处理
  - 只有RpycPythonEnv需要<id>metadata，因为它连接docker/utils/python_server.py
  - LocalPythonEnv, LocalPythonExecutor, AsyncPythonExecutor都不需要<id>metadata
  - worker.py和brain.py中会自动检测环境类型，只给RpycPythonEnv添加<id>：
    ```python
    # 只有RpycPythonEnv需要<id>标签
    if self.python_env.__class__.__name__ == 'RpycPythonEnv':
        code_with_id = f"<id>{self.input.query_id}/{self.input.run_id}</id>{code}"
        result = self.python_env.step(code_with_id)
    else:
        # 其他环境不需要<id>标签
        result = self.python_env.step(code)
    ```

- functions.final_answer调用修复
  - 修复了`functions.final_answer()`调用不抛异常的问题
  - 问题原因：`functions`命名空间中的`final_answer`是原始版本，不会抛出FinalAnswerException
  - 解决方案：在`evaluate_async_python_code`中创建异常包装器后，同时更新`functions`命名空间
  - 现在`functions.final_answer()`和直接调用`final_answer()`都会正确抛出异常并设置`is_final_answer=True`

- worker.py终止逻辑修复  
  - 修复了Python executor返回`is_final_answer=True`但任务不终止的问题
  - 问题原因：worker.py获取了`is_final_answer`值但没有使用，硬编码`terminated=False`
  - 解决方案：当`is_final_answer=True`时立即返回`terminated=True`的AgentResponse
  - 现在final_answer工具调用会正确终止任务执行

- Minion流式处理重构
  - 在基类Minion中添加了`stream_node_execution`通用方法
  - 所有子类现在使用统一的流式处理逻辑，直接yield StreamChunk对象
  - 移除了错误的final_answer检测逻辑，final_answer处理由LmpActionNode负责
  - 保持StreamChunk对象的原始结构，便于上层UI正确处理不同类型的chunk

- tool_choice参数支持
  - MinionToolCallingAgent和LmpActionNode现在都支持tool_choice参数
  - 可选值：
    - "auto": 让模型自动决定是否调用工具（默认值）
    - "none": 强制模型不调用任何工具
    - {"type": "function", "function": {"name": "function_name"}}: 强制调用特定工具
  - 使用示例：
    ```python
    # 在Agent中使用
    response = await agent.execute_step(state, tool_choice="none")
    
    # 在LmpActionNode中使用
    response = await lmp_node.execute(messages, tools=tools, tool_choice="auto")
    
    # 强制调用特定工具
    response = await lmp_node.execute(messages, tools=tools, 
                                    tool_choice={"type": "function", "function": {"name": "search"}})
    ```

- StreamChunk流式处理最佳实践
  - StreamChunk是agent流式输出的基本单元，每个chunk包含一小部分内容（如单个token）
  - UI处理时应该累积StreamChunk内容，而不是为每个chunk创建单独的消息行
  - StreamChunk有不同类型（chunk_type）：
    - 'text': Provider层输出文本（openai_provider使用）
    - 'tool_call': 工具调用信息
    - 'tool_response': 工具响应结果
    - 'llm_output': Agent层常规LLM输出内容
    - 'step_start': Agent步骤开始
    - 'step_end': Agent步骤结束
    - 'completion': 任务完成
    - 'warning': 警告信息
    - 'error': 错误信息
    - 'final_answer': 最终答案
  - 累积模式处理：
    ```python
    streaming_content = []
    if isinstance(event, StreamChunk):
        # 对于text和llm_output类型的chunk需要累积
        if event.chunk_type in ['text', 'llm_output']:
            streaming_content.append(event.content)
            display_content = "".join(streaming_content)
        else:
            # 其他类型单独处理（如error, final_answer等）
            handle_special_chunk(event)
    ```

### **开发流程记忆**
- 如果是一定功能的修改的话,尽可能添加test,先跑通test
- 如果非常简单的修改可以不用test
- 另外添加的文件请git add 到版本库里
- 如果是修改examples,也请改完之后试跑该example,修改必要的bug,直到跑通为止
