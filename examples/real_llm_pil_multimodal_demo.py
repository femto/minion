#!/usr/bin/env python3
"""
Real LLM + PIL.Image Multimodal Demo
====================================

This demo connects to actual LLM providers to test PIL.Image multimodal functionality.
Uses the existing minion config system like smart_minion/brain.py.

Usage:
    python examples/real_llm_pil_multimodal_demo.py
"""

import sys
import os
import asyncio

# Add project root to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from minion import config
from minion.main.brain import Brain
from minion.main.input import Input
from minion.providers import create_llm_provider


def create_test_images():
    """åˆ›å»ºæµ‹è¯•ç”¨çš„PILå›¾åƒ"""
    try:
        from PIL import Image, ImageDraw, ImageFont
        
        images = {}
        
        # 1. ç®€å•çš„å½©è‰²æ–¹å—
        img1 = Image.new('RGB', (200, 200), color='red')
        images['red_square'] = img1
        
        # 2. æ¸å˜å›¾åƒ
        img2 = Image.new('RGB', (200, 100))
        for x in range(200):
            for y in range(100):
                img2.putpixel((x, y), (int(255 * x / 200), int(255 * y / 100), 128))
        images['gradient'] = img2
        
        # 3. å¸¦æ–‡å­—çš„å›¾åƒ
        img3 = Image.new('RGB', (300, 100), color='white')
        draw = ImageDraw.Draw(img3)
        try:
            # å°è¯•ä½¿ç”¨é»˜è®¤å­—ä½“
            draw.text((10, 30), "Hello AI! Can you read this?", fill='black')
        except:
            # å¦‚æœæ²¡æœ‰å­—ä½“ï¼Œä½¿ç”¨é»˜è®¤
            draw.text((10, 30), "Hello AI! Can you read this?", fill='black')
        images['text_image'] = img3
        
        # 4. å‡ ä½•å›¾å½¢
        img4 = Image.new('RGB', (200, 200), color='lightblue')
        draw = ImageDraw.Draw(img4)
        # ç”»åœ†
        draw.ellipse([50, 50, 150, 150], fill='yellow', outline='black', width=2)
        # ç”»ä¸‰è§’å½¢
        draw.polygon([(100, 60), (80, 90), (120, 90)], fill='red')
        images['shapes'] = img4
        
        return images
        
    except ImportError:
        print("âš  PIL/Pillow not available. Please install: pip install Pillow")
        return {}


async def demo_basic_text_query(brain: Brain):
    """æ¼”ç¤ºåŸºæœ¬æ–‡æœ¬æŸ¥è¯¢"""
    print("\n" + "="*50)
    print("Demo 1: Basic Text Query")
    print("="*50)
    
    input_data = Input(
        query="Hello! Please introduce yourself briefly.",
        system_prompt="You are a helpful AI assistant."
    )
    
    try:
        result, _, _, _, _ = await brain.step(input_data)
        print(f"âœ“ LLM Response: {result}")
        return True
    except Exception as e:
        print(f"âŒ Basic text query failed: {e}")
        return False


async def demo_pil_image_query(brain: Brain, images: dict):
    """æ¼”ç¤ºPIL.Imageå¤šæ¨¡æ€æŸ¥è¯¢"""
    print("\n" + "="*50)
    print("Demo 2: PIL.Image Multimodal Query")
    print("="*50)
    
    if not images:
        print("âš  No images available, skipping PIL.Image test")
        return False
    
    # é€‰æ‹©ä¸€ä¸ªæµ‹è¯•å›¾åƒ
    test_image = images['red_square']
    
    input_data = Input(
        query=[
            "Please analyze this image:",
            test_image,
            "What color is it? What shape do you see?"
        ],
        system_prompt="You are an expert image analyst. Describe what you see in detail."
    )
    
    try:
        result, _, _, _, _ = await brain.step(input_data)
        print(f"âœ“ LLM Vision Response: {result}")
        return True
    except Exception as e:
        print(f"âŒ PIL.Image query failed: {e}")
        return False


async def demo_complex_multimodal_query(brain: Brain, images: dict):
    """æ¼”ç¤ºå¤æ‚çš„å¤šæ¨¡æ€æŸ¥è¯¢ï¼ˆå¤šå¼ å›¾ç‰‡+æ–‡æœ¬ï¼‰"""
    print("\n" + "="*50)
    print("Demo 3: Complex Multimodal Query (Multiple Images)")
    print("="*50)
    
    if len(images) < 2:
        print("âš  Need at least 2 images for complex test, skipping")
        return False
    
    # é€‰æ‹©ä¸¤å¼ ä¸åŒçš„å›¾åƒ
    image_names = list(images.keys())[:2]
    
    input_data = Input(
        query=[
            "I have two images to show you:",
            "Image 1:",
            images[image_names[0]],
            "Image 2:", 
            images[image_names[1]],
            "Can you compare these two images? What are the differences?"
        ],
        system_prompt="You are an expert at comparing and analyzing multiple images. Provide detailed comparisons."
    )
    
    try:
        result, _, _, _, _ = await brain.step(input_data)
        print(f"âœ“ Multi-image Response: {result}")
        return True
    except Exception as e:
        print(f"âŒ Complex multimodal query failed: {e}")
        return False


async def demo_text_image_query(brain: Brain, images: dict):
    """æ¼”ç¤ºå¸¦æ–‡å­—çš„å›¾åƒç†è§£"""
    print("\n" + "="*50)
    print("Demo 4: Text in Image Recognition")
    print("="*50)
    
    if 'text_image' not in images:
        print("âš  No text image available, skipping text recognition test")
        return False
    
    input_data = Input(
        query=[
            "Can you read the text in this image?",
            images['text_image'],
            "What does it say exactly?"
        ],
        system_prompt="You are excellent at reading text from images. Be precise about what text you see."
    )
    
    try:
        result, _, _, _, _ = await brain.step(input_data)
        print(f"âœ“ Text Recognition Response: {result}")
        return True
    except Exception as e:
        print(f"âŒ Text recognition query failed: {e}")
        return False


async def demo_image_file_path(brain: Brain):
    """æ¼”ç¤ºå›¾åƒæ–‡ä»¶è·¯å¾„æ”¯æŒ"""
    print("\n" + "="*50)
    print("Demo 5: Image File Path Support")
    print("="*50)
    
    # æ£€æŸ¥assetsç›®å½•ä¸­çš„å›¾åƒ
    asset_path = os.path.join(os.path.dirname(__file__), '..', 'assets', 'minion1.webp')
    
    if not os.path.exists(asset_path):
        print("âš  No asset image found, skipping file path test")
        return False
    
    input_data = Input(
        query=[
            "Please analyze this image file:",
            asset_path,
            "What do you see in this image? Describe it in detail."
        ],
        system_prompt="You are an image analyst. Describe the image in detail."
    )
    
    try:
        result, _, _, _, _ = await brain.step(input_data)
        print(f"âœ“ File Path Response: {result}")
        return True
    except Exception as e:
        print(f"âŒ File path query failed: {e}")
        return False


async def main():
    """ä¸»å‡½æ•°ï¼Œæ¨¡ä»¿smart_minion/brain.pyçš„é…ç½®æ–¹å¼"""
    print("Real LLM + PIL.Image Multimodal Demo")
    print("====================================")
    print("Using minion config system (like smart_minion/brain.py)")
    
    # 1. é…ç½®LLMï¼ˆæ¨¡ä»¿smart_minion/brain.pyï¼‰
    # ä½ å¯ä»¥æ›´æ”¹è¿™é‡Œçš„modelæ¥æµ‹è¯•ä¸åŒçš„LLM
    model = "gpt-4o-mini"  # æ”¯æŒvisionçš„æ¨¡å‹
    # model = "gpt-4o"     # æ›´å¥½çš„visionä½†æ›´è´µ
    # model = "claude"     # Claude 3.5 Sonnet
    # model = "gemini-2.0-flash-exp"  # Gemini
    
    print(f"ğŸ¤– Using model: {model}")
    
    try:
        llm_config = config.models.get(model)
        if not llm_config:
            print(f"âŒ Model '{model}' not found in config!")
            print("Available models:", list(config.models.keys()))
            return 1
        
        llm = create_llm_provider(llm_config)
        print(f"âœ“ Created LLM provider: {type(llm).__name__}")
        print(f"âœ“ Model: {llm_config.model}")
        print(f"âœ“ API Type: {llm_config.api_type}")
        
    except Exception as e:
        print(f"âŒ Failed to create LLM provider: {e}")
        return 1
    
    # 2. åˆ›å»ºBrainï¼ˆä½¿ç”¨LocalPythonEnvé¿å…Dockerä¾èµ–ï¼‰
    try:
        brain = Brain(llm=llm)  # é»˜è®¤ä½¿ç”¨LocalPythonEnv
        print(f"âœ“ Created Brain with {type(brain.python_env).__name__}")
    except Exception as e:
        print(f"âŒ Failed to create Brain: {e}")
        return 1
    
    # 3. åˆ›å»ºæµ‹è¯•å›¾åƒ
    print("\nğŸ¨ Creating test images...")
    images = create_test_images()
    if images:
        print(f"âœ“ Created {len(images)} test images: {list(images.keys())}")
    else:
        print("âš  No images created (PIL not available)")
    
    # 4. è¿è¡Œæ¼”ç¤º
    print("\nğŸ§ª Running multimodal demos with real LLM...")
    
    demo_results = []
    
    # åŸºæœ¬æ–‡æœ¬æ¼”ç¤º
    result = await demo_basic_text_query(brain)
    demo_results.append(("Basic Text", result))
    
    # PIL.Imageæ¼”ç¤ºï¼ˆéœ€è¦visionæ¨¡å‹ï¼‰
    if images:
        result = await demo_pil_image_query(brain, images)
        demo_results.append(("PIL.Image", result))
        
        result = await demo_complex_multimodal_query(brain, images)
        demo_results.append(("Multi-Image", result))
        
        result = await demo_text_image_query(brain, images)
        demo_results.append(("Text Recognition", result))
    
    # æ–‡ä»¶è·¯å¾„æ¼”ç¤º
    result = await demo_image_file_path(brain)
    demo_results.append(("File Path", result))
    
    # 5. æ€»ç»“ç»“æœ
    print("\n" + "="*50)
    print("Demo Results Summary")
    print("="*50)
    
    success_count = 0
    for demo_name, success in demo_results:
        status = "âœ… PASS" if success else "âŒ FAIL"
        print(f"{demo_name:15} {status}")
        if success:
            success_count += 1
    
    print(f"\nTotal: {success_count}/{len(demo_results)} demos succeeded")
    
    if success_count == len(demo_results):
        print("\nğŸ‰ All demos passed! PIL.Image multimodal support is working with real LLM!")
    elif success_count > 0:
        print("\nâš ï¸  Some demos passed. Check your LLM's vision capabilities.")
        if success_count == 1 and demo_results[0][1]:  # åªæœ‰åŸºæœ¬æ–‡æœ¬æ¼”ç¤ºé€šè¿‡
            print("ğŸ’¡ Your LLM works but may not support vision. Try a vision-enabled model like:")
            print("   - gpt-4o-mini")
            print("   - gpt-4o") 
            print("   - claude")
    else:
        print("\nâŒ All demos failed. Check your LLM configuration.")
    
    # 6. é¢å¤–æ¼”ç¤ºï¼šå¿«é€Ÿè®¡ç®—æ¼”ç¤ºï¼ˆç±»ä¼¼smart_minion/brain.pyï¼‰
    print("\n" + "="*50)
    print("Bonus: Quick Math Demo (like smart_minion/brain.py)")
    print("="*50)
    
    try:
        result, _, _, _, _ = await brain.step(
            query="what's the solution 234*568",
            route="python",
            check=False
        )
        print(f"âœ“ Math Result: {result}")
    except Exception as e:
        print(f"âŒ Math demo failed: {e}")
    
    return 0 if success_count > 0 else 1


if __name__ == "__main__":
    try:
        exit_code = asyncio.run(main())
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  Demo interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâŒ Demo failed with error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1) 