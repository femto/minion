#!/usr/bin/env python3
"""
åŸºäº minion OpenAI provider çš„åŒæ­¥æµå¼èŠå¤©å®Œæˆç¤ºä¾‹
"""

from minion import config
from minion.providers import create_llm_provider

def stream_chat_example():
    """æ¼”ç¤ºåŒæ­¥æµå¼èŠå¤©å®Œæˆ"""
    
    # è·å– LLM é…ç½®å’Œ provider
    model = "gpt-4o"  # æˆ–è€…ä½ æƒ³ç”¨çš„å…¶ä»–æ¨¡å‹
    llm_config = config.models.get(model)
    llm = create_llm_provider(llm_config)
    
    # è·å–åŒæ­¥å®¢æˆ·ç«¯
    client = llm.client_sync
    
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚"},
        {"role": "user", "content": "è¯·å†™ä¸€é¦–å…³äºç¼–ç¨‹çš„çŸ­è¯—"}
    ]
    
    print("å¼€å§‹æµå¼å“åº”:")
    print("-" * 50)
    
    # åˆ›å»ºæµå¼èŠå¤©å®Œæˆ
    stream = client.chat.completions.create(
        model=llm.config.model,  # ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹
        messages=messages,
        stream=True,  # å¯ç”¨æµå¼å“åº”
        max_tokens=200,
        temperature=0.7
    )
    
    # é€æ­¥å¤„ç†æµå¼å“åº”
    full_response = ""
    for chunk in stream:
        # æ£€æŸ¥æ˜¯å¦æœ‰å†…å®¹
        if chunk.choices and chunk.choices[0].delta.content is not None:
            content = chunk.choices[0].delta.content
            print(content, end="", flush=True)  # å®æ—¶æ‰“å°ï¼Œä¸æ¢è¡Œ
            full_response += content
    
    print("\n" + "-" * 50)
    print(f"å®Œæ•´å“åº”: {full_response}")
    print(f"å“åº”é•¿åº¦: {len(full_response)} å­—ç¬¦")

def stream_with_error_handling():
    """å¸¦é”™è¯¯å¤„ç†çš„æµå¼å“åº”ç¤ºä¾‹"""
    
    # è·å– LLM é…ç½®å’Œ provider
    model = "gpt-4o"
    llm_config = config.models.get(model)
    llm = create_llm_provider(llm_config)
    client = llm.client_sync
    
    try:
        stream = client.chat.completions.create(
            model=llm.config.model,
            messages=[
                {"role": "user", "content": "è§£é‡Šä»€ä¹ˆæ˜¯é€’å½’"}
            ],
            stream=True,
            max_tokens=150
        )
        
        collected_messages = []
        
        for chunk in stream:
            if chunk.choices and chunk.choices[0].delta.content is not None:
                chunk_message = chunk.choices[0].delta.content
                collected_messages.append(chunk_message)
                print(chunk_message, end="")
        
        print(f"\n\næ”¶é›†åˆ° {len(collected_messages)} ä¸ªæ¶ˆæ¯å—")
        
    except Exception as e:
        print(f"å‘ç”Ÿé”™è¯¯: {e}")

def simple_stream_example():
    """æœ€ç®€å•çš„æµå¼ç¤ºä¾‹"""
    
    # æ¨¡ä»¿ brain.py çš„æ–¹å¼
    model = "gpt-4o"
    llm_config = config.models.get(model)
    llm = create_llm_provider(llm_config)
    
    # ç›´æ¥ä½¿ç”¨åŒæ­¥å®¢æˆ·ç«¯è¿›è¡Œæµå¼è°ƒç”¨
    stream = llm.client_sync.chat.completions.create(
        model=llm.config.model,
        messages=[{"role": "user", "content": "ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯AI"}],
        stream=True,
        max_tokens=100
    )
    
    print("AI å›ç­”: ", end="", flush=True)
    for chunk in stream:
        if chunk.choices and chunk.choices[0].delta.content is not None:
            print(chunk.choices[0].delta.content, end="", flush=True)
    print("\n")

def stream_with_usage_example():
    """æ¼”ç¤ºå¦‚ä½•è·å–æµå¼å“åº”ä¸­çš„ usage ä¿¡æ¯"""
    
    model = "gpt-4o"
    llm_config = config.models.get(model)
    llm = create_llm_provider(llm_config)
    client = llm.client_sync
    
    # åˆ›å»ºæµå¼èŠå¤©å®Œæˆï¼Œå¯ç”¨ usage ç»Ÿè®¡
    stream = client.chat.completions.create(
        model=llm.config.model,
        messages=[{"role": "user", "content": "è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Œç”¨50å­—ä»¥å†…"}],
        stream=True,
        max_tokens=100,
        #stream_options={"include_usage": True}  # å…³é”®ï¼šå¯ç”¨ usage ç»Ÿè®¡
    )
    
    print("æµå¼å“åº”: ", end="", flush=True)
    full_response = ""
    usage_info = None
    
    for chunk in stream:
        # å¤„ç†å†…å®¹
        if chunk.choices and chunk.choices[0].delta.content is not None:
            content = chunk.choices[0].delta.content
            print(content, end="", flush=True)
            full_response += content
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ usage ä¿¡æ¯ï¼ˆé€šå¸¸åœ¨æœ€åä¸€ä¸ª chunkï¼‰
        if hasattr(chunk, 'usage') and chunk.usage is not None:
            usage_info = chunk.usage
            print(f"\n\nğŸ“Š Usage ä¿¡æ¯:")
            print(f"  Prompt tokens: {usage_info.prompt_tokens}")
            print(f"  Completion tokens: {usage_info.completion_tokens}")
            print(f"  Total tokens: {usage_info.total_tokens}")
    
    print(f"\n\nå®Œæ•´å“åº”: {full_response}")
    
    if usage_info is None:
        print("âš ï¸  æœªè·å–åˆ° usage ä¿¡æ¯ï¼Œå¯èƒ½éœ€è¦æ£€æŸ¥ API ç‰ˆæœ¬æˆ–å‚æ•°è®¾ç½®")

if __name__ == "__main__":
    print("åŸºäº minion OpenAI provider çš„åŒæ­¥æµå¼èŠå¤©ç¤ºä¾‹")
    print("=" * 60)
    
    # è¿è¡Œæœ€ç®€å•çš„ç¤ºä¾‹
    #simple_stream_example()
    
    print("=" * 60)
    
    # è¿è¡Œå¸¦ usage ç»Ÿè®¡çš„ç¤ºä¾‹
    stream_with_usage_example()
    
    print("\n" + "=" * 60)
    
    # è¿è¡ŒåŸºæœ¬ç¤ºä¾‹
    stream_chat_example()
    
    print("\n" + "=" * 60)
    
    # è¿è¡Œå¸¦é”™è¯¯å¤„ç†çš„ç¤ºä¾‹
    stream_with_error_handling()